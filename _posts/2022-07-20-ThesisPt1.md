---
layout: post
title: Splendidly Lifting Equivalences - Explaining My Thesis To A (More) General Audience
subtitle: Part 1 - Just What Exactly Is A Representation? 
---

Hello and welcome! I'm Sam, a Ph.D. candidate studying modular representation theory - in particular, equivalences between representation spaces. If you don't know what those words mean, fear not. This series of blog posts will attempt to introduce my field and more! My end goal is to build up to painting a picture of the question which guides my dissertation, getting into some, but not too many, technical details. The intended audience is an undergraduate student who's taken a course on group and ring theory, with basic linear algebra assumed as well. Though if you're not there yet, don't worry! You still might get something out of these posts, and I might write an appendix-like primer for everything you need from those courses to get what I'm doing here.  

# What is a Representation?

We've got a long way to go to get to the meat of my dissertation problem, but just like anything worth pursuing, we've gotta start small to get to the juicy stuff. I'll assume that you need no refresher on what a matrix, determinant, group, group homomorphism, isomorphism, or ring is, but it may be beneficial to remind you of a few things: 

- A **field** is a commutative ring where every nonzero element has a multiplicative inverse. Perhaps the most standard examples of fields are the rational, real, and complex numbers, denoted $\mathbb{Q}, \mathbb{R},$ and $\mathbb{C}$ respectively. On the other hand, the integers, $\mathbb{Z}$ are *not* a field, as any integer besides $-1$ and $1$ is non-invertible. For the rest of this post, I will write $k$ to denote some arbitrary field - at least for the first half of this post, it's perfectly fine to just substitute in, say, $\mathbb{C}$! (When we get to discussing representations, its best to substitute $\mathbb{C}$ rather than $\mathbb{R}$ or $\mathbb{Q}$ for reasons we will not discuss today.)

- An abstract **vector space over $k$** is, roughly speaking, an (additive) abelian group that also has a well-behaved scalar multiplication by elements of $k$. Recall that any vector space over $k$ which has finite dimension, say it has dimension $n$, is in fact isomorphic to $k^n$, that is, the vector space given by $n\times 1$ matrices: 

    $$\left\{\begin{pmatrix} a_1 \\\ \vdots \\\ a_n \end{pmatrix}: a_1,\dots, a_n \in k \right\}.$$

    So, it won't hurt us too much to assume that any finite dimensional vector space over $k$ actually just $k^n$ for some positive integer $n$! Some classic examples of vector spaces (over $\mathbb{R}$) are $\mathbb{R}^2$, e.g. a 2D plane such as the screen you're reading this post off of (well, assuming that your screen has infinite precision and is boundaryless), or $\mathbb{R}^3$, the standard space we're used to navigating around in our lives.
    
- A **vector space homomorphism** is a map between vector spaces over $k$ which respects vector addition and scalar multiplication. Recall that all vector space homomorphisms are expressible by matrices with its entries being elements in $k$, and conversely, every matrix generates a vector space homomorphism!   
    
    We say a vector space homomorphism is an **endomorphism** if it is a map from a vector space to itself - in this case it'll be given by a square matrix. Say an endomorphism is a **automorphism** if it is invertible - in this case, the corresponding matrix will have nonzero determinant, and a multiplicative inverse, which corresponds to the inverse of the homomorphism.  

We'll denote the set of vector space automorphisms of a vector space $V$ by $\text{Aut}(V)$. Note that $\text{Aut}(V)$ forms a group, where the binary operation is given by composition of functions. Now, we can finally define what a representation is!

**Definition:** Let $G$ be a group and $V$ be a finite-dimensional vector space over a field $k$. A (group) **representation** $\rho$ **over** $k$ is a group homomorphism

$$\rho: G \to \text{Aut}(V).$$

Okay, maybe that's not the most enlightening definition, but bear with me. First of all, looking back to what I noted about how there is a pairing between vector space automorphisms and matrices with nonzero determinant, we can state this more formally in the following way: we write $GL_n(k)$ (GL short for "general linear") for the group of $n\times n$ invertible matrices, under matrix multiplication. Then there is a group isomorphism 

$$\text{Aut}(V) \cong GL_n(k).$$ 

So it won't hurt us to redefine what a representation is for the time being.

**(Equivalent) ReDefinition:** Let $G$ be a group, $n$ a positive integer, and $k$ a field. A **representation** $\rho$ **over** $k$ is a group homomorphism 

$$\rho: G \to GL_n(k).$$

So to paint an informal picture, to create a representation, for every group element in $G$, you choose a $n \times n$ matrix, but with the additional stipulation that group multiplication corresponds to matrix multiplication. If you're weirded out that I'm giving two different definitions, take some comfort in knowing that these are indeed equivalent definitions. In fact, in my undergraduate representation theory course, I saw the latter definition given as the first definition, whereas in my first graduate representation theory course, I was only given the former. But in general, I'd say the former is more precise, as there isn't a *canonical* isomorphism $\text{Aut{(V) \cong GL_n(k)$, it depends on the choice of basis. The latter is more useful for vizualization. 

**Example:** Let $C_2 = \\{1, \sigma\\}$ be the group with 2 elements. Then a representation of $C_2$ over $\mathbb{R}$ is given by the assignment: 

$$1 \mapsto \begin{pmatrix} 1 & 0 \\\ 0 & 1 \end{pmatrix}, \qquad \sigma \mapsto \begin{pmatrix} 0 & 1 \\\ -1 & 0 \end{pmatrix}.$$ 

Let's double check to make sure the mapping respects the group structure: since $\sigma^2 = 1$, it should also be true that the corresponding matrix is its own inverse. Indeed, 

$$\begin{pmatrix} 0 & 1 \\\ -1 & 0 \end{pmatrix}\cdot \begin{pmatrix} 0 & 1 \\\ -1 & 0 \end{pmatrix} = \begin{pmatrix} 1 & 0 \\\ 0 & 1 \end{pmatrix}.$$

Now if you're perceptive, you may have noticed that the matrix that $\sigma$ is mapped to is the rotation matrix: 

$$\begin{pmatrix} \cos \pi  & \sin \pi \\\ -\sin \pi & \cos\pi \end{pmatrix},$$ 

i.e. the matrix which rotates a vector $(x,y) \in \mathbb{R}^2$ halfway around the origin! Generalizing this construction a bit, say $C_k = \langle \sigma \rangle$ is the cyclic group of $k$ elements, and is generated by $\sigma$. Then, a representation of $C_k$ will be completely determined by the assignment: 

$$\sigma \mapsto \begin{pmatrix} \cos 2\pi/k  & \sin 2\pi/k \\\ -\sin 2\pi/k & \cos 2\pi/k \end{pmatrix}.$$

I'll leave it as an exercise to the reader to verify that this is indeed a representation. There's only one thing that actually needs verifying! 

**Example:** For any group $G$, the representation sending $g \mapsto (1)$ for all $g \in G$ is called the **trivial representation**. 

The justification I give to non-math types when they ask me what representations are useful for is that they give us ways of, well, representing abstract objects in a concrete, computation-friendly way. However, this is a bit of a lie - I'm not a math historian but I don't believe this was any motivation at all of studying representations historically (in fact, one can study groups via computer using GAP), and while representation theory is a powerful tool for deriving purely group-theoretic results (such as **Burnside's** $p^aq^b$ **theorem**, which states that finite groups which have order $p^aq^b$ for primes $p,q$ are solvable), these results have nothing to do with computers. At least for me, representation theory gives a much more geometric approach to studying groups via the way they can act on vector spaces. 

# Modules and the Group Algebra

We'll now step away from representations for a brief minute to introduce a new concept - (left) $R$-*modules* (just "modules" for short). In my view, a module is essentially a generalization of a vector space, and in fact, all vector spaces are examples of modules! I was debating trying to loosely describe what a module is, but because they are so integral to what we'll be discussing, I think it's best if I give the full definition. 

**Definition:** Let $R$ be a ring with identity. A **left $R$-module** $M$ is an abelian group $(M, +)$ that has a "left multiplication" by $R$,'' i.e. an operation 

$$\cdot \,: R \times M \to M$$

which satisfies, for all $r, s \in R$ and $m,n \in M$,

- (Distributivity over $M$) $r \cdot (x + y) = r\cdot x + r\cdot y.$
- (Distributivity over $R$) $(r + s) \cdot x = r\cdot x + s\cdot x.$
- (Associativity) $(rs)\cdot x = r\cdot (s\cdot x).$
- (Unit acts trivially) $1\cdot x = x.$

This might look a bit similar to the formal definition of a vector space. In fact, if $R$ is a field, then $M$ is a $R$-module if and only if $M$ is a vector space over $R$ - the notions coincide! The difference here is that $R$ need not be a field. As another quick example, one can show that the $\mathbb{Z}$-modules are precisely all the abelian groups - we'll leave that as an exercise for the reader. 

With $R$-modules come $R$-module homomorphisms. I won't get into the formal definition as I don't think we'll need it, but they're essentially the analogue to group homomorphisms and vector space homomorphisms, in that they respect the structure of the module additively, and that the $R$-multiplication factors through the homomorphism. 

Now you may be asking, "Why are we talking about modules? I thought this was a discussion about representations!" Well, there's a good reason that I'm bringing this up, but we're not ready to rationalize it just yet. The construction that ties these two notions together is a ring (and a vector space) called the *group algebra*. Essentially, this is a construction that combines the data of a group and a commutive ring (such as a field) together.

*For the rest of this entire blog series, unless stated otherwise, any group is assumed to be* ***finite***. (The group algebra can be defined over infinite groups as well, however there are more nuances that I don't want to get into.)

**Definition:** Let $G$ be a finite group and $R$ a commutative ring. The **group algebra of** $G$ **over** $R$, $R[G]$ (or just $RG$) is the ring given by formal linear combinations of elements of $G$ over $R$ (in other words, for each group element, associate a scalar in $R$ to it). Explicitly, elements look like finite sums of the form: 

$$R[G] = \left\{\sum_{g \in G} a_g\cdot g : a_g \in R\right\}.$$ 

Addition is performed componentwise and multiplication is given by linearly extending the multiplication of $G$. 

**Example:** Let $R = \mathbb{Z}$ and $G = C_2$, the two-element group $\\{1, \sigma\\}$ from before. Then all elements will look like: $$a_1 \cdot 1 + a_\sigma \cdot \sigma \in \mathbb{Z} C_2, \quad a_1, a_\sigma \in \mathbb{Z}.$$ Remember, the "1" is an element in $C_2$, not a scalar! Let's take two example elements, $3\cdot 1 + 2\cdot \sigma, 2\cdot 1 - 4\cdot \sigma \in \mathbb{Z} C_2$ and compute their sum and product:

$$(3\cdot 1 + 2\cdot \sigma) + (2\cdot 1 - 4\cdot \sigma) = 5\cdot 1 - 2\cdot \sigma.$$

$$\begin{aligned}(3\cdot 1 + 2\cdot \sigma) \times (2\cdot 1 - 4\cdot \sigma) &= 6\cdot (1\cdot 1) - 12\cdot (1\cdot \sigma) +4\cdot (\sigma\cdot 1) - 8\cdot (\sigma \cdot \sigma)\\\ &= -2\cdot 1 - 8 \cdot \sigma.\end{aligned}$$

Hopefully this example helps you get some intuition for how a group algebra looks and works! 

# What are Representations, *again?*

Let's go back to our original definition of representations, group homomorphisms $\rho: G \to \text{Aut}(V)$ for some $k$-vector space $V$. As it turns out, the existence of some representation $\rho$ enables us to turn $V$, which remember, is also a $k$-module, *into a $kG$-module!*

How does this work? Well first, let's note that we don't need to define the $kG$ multiplication on $V$ - we only need to define a $G$-multiplication on $V$, then the distributive laws of modules take care of the rest! So how do we define a $G$ multiplication on $V$? 

Well, given some $g \in G$, $\rho(G)$ is an automorphism of $V$! So we can define the multiplication as follows: 

$$g\cdot v := \rho(g)(v).$$ 

Remember, $\rho(g)$ is a function! Now one has to prove that this is indeed a module, that's not immediate! But I'll leave that as an exercise to the reader. The long and short of it is that given a representation $\rho$, we get a $kG$-module $V$ for free. So representations give us $kG$-modules. Cool!

But things gets even better - we can go the other way too. Given a $kG$-module $M$, we can turn $M$ into a representation $\rho_M: G \to \text{Aut}(M)$. How so? Well, we can first *restrict* the scalars of $M$ from $kG$ to $k$, turning $M$ into a $k$-vector space (how the restriction works is a bit more complicated, but let's not get into details). Then, looking at the action of $G$-multiplication on $M$, one can show that multiplication by any $g \in G$ induces a $k$-vector space automorphism on $M$! So we define the representation by sending $g$ to the automorphism induced by multiplication by $g$, that is, 

$$g \mapsto (m \mapsto g\cdot m).$$ 

Again, this all needs proof, but again, I'll leave it as an exercise to the reader. So given a $kG$-module $M$, we get a representation back! 

Now here's the kicker - these two constructions I've just described are *mutually inverse*. So we've constructed a bijection between the set of representations over $k$ of a finite group $G$ and the set of $kG$-modules. So, it won't hurt to redefine what a representation is *again*, will it? After all, we can just turn it back into a representation in the original sense if need be. 

**(Equivalent) ReReDefinition:** A **representation of $G$ over $k$** is a $kG$-module. 

Now there are some technicalities here - this way of viewing representations as far as I know is only used in the case where $G$ is a finite group. The representation theory of infinite groups (such as profinite and Lie groups) is vastly different, and not my specialty. Additionally, I've assumed (without stating) that both the vector spaces and $kG$-modules are finite-dimensional (for the latter, in the sense that they have finite $k$-dimension, or alternatively, that they are finitely generated, but let's not get into that). 

For the finite group case, we have the power of module theory at our disposal when studying representations! In the case of my work, I basically use the module definition of a representation as the standard definition, as I only work over finite groups, though there are a few times when it's convenient to switch back for computational purposes, especially in the case of working with permutation representations, that is, $kG$-modules of the form $k[X]$, where $X$ is a $G$-set. 

We'll leave things at that for today - I've basically summed up what might be the first week or two of a full course in representation theory in a shortish blog post, so if this felt fast, don't worry. As things go on, I anticipate things getting slightly less technical as well - as we approach Category Theory, Grothendieck groups, and so on, I think I'll be able to use more handwavey, but simultaneously more intuitive, explanations. Next time, I'll talk about the differences of representation theory over $\mathbb{C}$ (the classical case, usually what one encounters in a first course in representation theory) and *modular* representation theory, that is, representation theory over fields of nonzero characteristic! Slight spoiler: things break. **Horrifically.**
